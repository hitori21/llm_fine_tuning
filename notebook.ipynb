{"cells":[{"cell_type":"markdown","source":["# **LLM**\n","The Large Language Model (LLM) is a deep learning-based language model with large and complex parameters. This model goes through a pre-training stage on big and diverse data before being featured or finetuned to a specific task."],"metadata":{"id":"JVrnnOcj0-Zt"}},{"cell_type":"markdown","source":["# **Fine-Tuning**\n","Fine-tuning is the process of taking a pre-trained machine learning model and retraining (tuning) it partially or completely on a smaller dataset and a more specific task. The aim is to improve the model's performance in more specific tasks by leveraging the knowledge gained during previous training. With fine-tuning, models can be adapted to better handle specific tasks without having to train from scratch, which often requires greater time and resources."],"metadata":{"id":"dnwfDuvT0TsN"}},{"cell_type":"markdown","source":["# **0.0 Using Hugging Face Dataset**\n","Fine-tuning a Language Model (LLM) involves the process of refining a pre-trained model using a specific dataset provided by Hugging Face. This dataset serves as additional training data, allowing the model to adapt and specialize its language understanding to the particular domain or task represented by the dataset."],"metadata":{"id":"jIx-FK4bfTld"}},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"GTDbJJnDgEzj"},"outputs":[],"source":["#@title 0.1 Mount Drive\n","\n","import os\n","from google.colab import drive\n","\n","#os.mkdir(\"Drive\")\n","drive.mount(\"Drive\")\n","\n","print(\"[INFO] Success mount drive at /content/Drive\")"]},{"cell_type":"code","source":["#@title 0.2 Install Transformers from GitHub\n","!git clone https://github.com/huggingface/transformers\n","%cd transformers\n","!python setup.py install\n","%cd /content/transformers/examples/pytorch/language-modeling\n","\n","print(\"[INFO] Success build Transformers\")"],"metadata":{"cellView":"form","id":"UFScELwvgK2R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title 0.3 Fine-Tuning Model\n","#@markdown Choose a natural language processing pretrained model or enter your last checkpoint folder path (if you wanna resume training from checkpoint). It is recommended to use a model with small parameters if you are using a free GPU\n","MODEL = \"openai-gpt\" #@param [\"openai-gpt\", \"distilgpt2\", \"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\", \"facebook/opt-125m\", \"facebook/opt-350m\", \"facebook/opt-2.7b\", \"facebook/xglm-564M\", \"facebook/xglm-7.5B\", \"EleutherAI/gpt-neo-125m\", \"EleutherAI/gpt-neo-1.3B\", \"EleutherAI/gpt-j-6b\", \"EleutherAI/pythia-14m\", \"EleutherAI/pythia-31m\", \"EleutherAI/pythia-70m\", \"EleutherAI/pythia-160m\", \"EleutherAI/pythia-410m\" , \"EleutherAI/pythia-1b\", \"roneneldan/TinyStories-1M\", \"roneneldan/TinyStories-3M\", \"roneneldan/TinyStories-8M\", \"roneneldan/TinyStories-33M\"] {allow-input: true}\n","#@markdown Enter the model name for the output (without spaces, or replace using \"-\")\n","OUTPUT_NAME = \"MikaGPT\" #@param {type: \"string\"}\n","#@markdown Select the dataset you want to use\n","DATASET_NAME = \"yahma/alpaca-cleaned\" #@param [\"wikitext:wikitext-103-raw-v1\", \"wikitext:wikitext-103-v1\", \"wikitext:wikitext-2-raw-v1\", \"wikitext:wikitext-2-v1\", \"yahma/alpaca-cleaned\"]\n","#@markdown Enter a number for the number of steps for each stored checkpoint. Enter -1 if you want to save only the last checkpoint\n","SAVE_STEPS = 100 #@param {type: \"integer\"}\n","#@markdown Enter the batch size for each device. Batch size is the sample dataset that is given to the model in one iteration. The more batch sizes, the smaller the gradient variations and the greater the GPU usage. On the other hand, if it is too small, the gradient variation will increase but may cause convergence\n","BATCH_SIZE = 11 #@param {type:\"slider\", min:1, max:64, step:1}\n","#@markdown Enter the number of Epochs for the Fine-Tuning process. Epochs are the number of iterations of one dataset given to the model. The more, the process will take longer\n","TRAIN_EPOCHS = 3 #@param {type:\"slider\", min:1, max:100, step:1}\n","#@markdown Select if you want to continue model training from the checkpoint\n","RESUME_FROM_CHECKPOINT = False #@param {type:\"boolean\"}\n","#@markdown Enter additional parameters, leave blank if there are no additional parameters. For documentation please run cell 0.5 or 1.6\n","ADDITIONAL_PARAMETERS = \"\" #@param {type: \"string\"}\n","\n","if RESUME_FROM_CHECKPOINT:\n","  ADDITIONAL_PARAMETERS += f\" --resume_from_checkpoint {MODEL}\"\n","\n","import re\n","\n","if re.search(\"wikitext\", DATASET_NAME):\n","  DATASET = DATASET_NAME.split(\":\")\n","  ADDITIONAL_PARAMETERS += f\" --dataset_name {DATASET[0]} --dataset_config_name {DATASET[1]}\"\n","else:\n","   ADDITIONAL_PARAMETERS += f\" --dataset_name {DATASET_NAME}\"\n","\n","!pip install -r requirements.txt\n","!python run_clm.py --model_name_or_path {MODEL} --per_device_train_batch_size {BATCH_SIZE} --per_device_eval_batch_size {BATCH_SIZE} --do_train --do_eval --output_dir /content/{OUTPUT_NAME} --overwrite_output_dir --save_steps={SAVE_STEPS} --num_train_epochs {TRAIN_EPOCHS} --logging_steps 100 --save_total_limit 2 {ADDITIONAL_PARAMETERS}\n","\n","print(\"[INFO] Success Fine Tuning Model\")"],"metadata":{"cellView":"form","id":"R8LUm6X9gRWY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title 0.4 Save data to Drive\n","#@markdown Enter the checkpoint folder name (example: checkpoint-10000). Enter \"-\" if you only want to save the last checkpoint\n","STEP = \"-\" #@param {type: \"string\"}\n","\n","%cd ../../../..\n","!mkdir /content/Drive/MyDrive/{OUTPUT_NAME}\n","\n","if STEP != \"-\":\n","  !cp -r {OUTPUT_NAME}/{STEP}/config.json /content/Drive/MyDrive/{OUTPUT_NAME}\n","  !cp -r {OUTPUT_NAME}/{STEP}/generation_config.json /content/Drive/MyDrive/{OUTPUT_NAME}\n","  !cp -r {OUTPUT_NAME}/{STEP}/merges.txt /content/Drive/MyDrive/{OUTPUT_NAME}\n","  !cp -r {OUTPUT_NAME}/{STEP}/pytorch_model.bin /content/Drive/MyDrive/{OUTPUT_NAME}\n","  !cp -r {OUTPUT_NAME}/{STEP}/tokenizer.json /content/Drive/MyDrive/{OUTPUT_NAME}\n","  !cp -r {OUTPUT_NAME}/{STEP}/vocab.json /content/Drive/MyDrive/{OUTPUT_NAME}\n","else:\n","  !cp -r {OUTPUT_NAME}/config.json /content/Drive/MyDrive/{OUTPUT_NAME}\n","  !cp -r {OUTPUT_NAME}/generation_config.json /content/Drive/MyDrive/{OUTPUT_NAME}\n","  !cp -r {OUTPUT_NAME}/merges.txt /content/Drive/MyDrive/{OUTPUT_NAME}\n","  !cp -r {OUTPUT_NAME}/pytorch_model.bin /content/Drive/MyDrive/{OUTPUT_NAME}\n","  !cp -r {OUTPUT_NAME}/tokenizer.json /content/Drive/MyDrive/{OUTPUT_NAME}\n","  !cp -r {OUTPUT_NAME}/vocab.json /content/Drive/MyDrive/{OUTPUT_NAME}\n","\n","print(\"[INFO] Success saving data to Google Drive\")"],"metadata":{"cellView":"form","id":"f_aOwAKBlA-Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title 0.5 Show help for additional parameters\n","!pip install -r requirements.txt\n","!python run_clm.py --help"],"metadata":{"cellView":"form","id":"H16xa54DlHwj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **1.0 Using Custom Dataset**\n","Fine-tuning a Large Language Model (LLM) with a custom text dataset involves adapting a pre-trained model by training it on your own dataset. This process aims to make the model more specific to a particular task or domain."],"metadata":{"id":"wDw-qjupSPnb"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"HiFnZbE8uRuH","cellView":"form"},"outputs":[],"source":["#@title 1.1 Mount Drive\n","\n","import os\n","from google.colab import drive\n","\n","#os.mkdir(\"Drive\")\n","drive.mount(\"Drive\")\n","\n","print(\"[INFO] Success mount drive at /content/Drive\")"]},{"cell_type":"code","source":["#@title 1.2 Install Transformers from GitHub\n","!git clone https://github.com/huggingface/transformers\n","%cd transformers\n","!python setup.py install\n","%cd /content/transformers/examples/pytorch/language-modeling\n","\n","print(\"[INFO] Success build Transformers\")"],"metadata":{"id":"B2kshDTBDXQF","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title 1.3 Splitting the text dataset (TXT) into two files, training and validation\n","#@markdown Enter the file path dataset text that will be used\n","DATASET_RAW = \"/content/Drive/MyDrive/LN-ID-10K/LN-ID-10K-10000-ch.txt\" #@param {type: \"string\"}\n","#@markdown Enter the size of the training file\n","TRAIN_SIZE = 0.85 #@param {type:\"slider\", min:0.5, max:0.95, step:0.01}\n","\n","from sklearn.model_selection import train_test_split\n","\n","with open(DATASET_RAW, 'r') as file:\n","    dataset = file.readlines()\n","\n","train_data, valid_data = train_test_split(dataset, train_size=TRAIN_SIZE, random_state=42)\n","\n","with open('train.txt', 'w') as file:\n","    file.writelines(train_data)\n","\n","with open('valid.txt', 'w') as file:\n","    file.writelines(valid_data)\n","\n","print(\"[INFO] Success splitting data\")"],"metadata":{"id":"ZkJl-Hy06_cs","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title 1.4 Fine-Tuning Model\n","#@markdown Choose a natural language processing pretrained model or enter your last checkpoint folder path (if you wanna resume training from checkpoint). It is recommended to use a model with small parameters if you are using a free GPU\n","MODEL = \"openai-gpt\" #@param [\"openai-gpt\", \"distilgpt2\", \"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\", \"facebook/opt-125m\", \"facebook/opt-350m\", \"facebook/opt-2.7b\", \"facebook/xglm-564M\", \"facebook/xglm-7.5B\", \"EleutherAI/gpt-neo-125m\", \"EleutherAI/gpt-neo-1.3B\", \"EleutherAI/gpt-j-6b\", \"EleutherAI/pythia-14m\", \"EleutherAI/pythia-31m\", \"EleutherAI/pythia-70m\", \"EleutherAI/pythia-160m\", \"EleutherAI/pythia-410m\" , \"EleutherAI/pythia-1b\", \"roneneldan/TinyStories-1M\", \"roneneldan/TinyStories-3M\", \"roneneldan/TinyStories-8M\", \"roneneldan/TinyStories-33M\"] {allow-input: true}\n","#@markdown Enter the model name for the output (without spaces, or replace using \"-\")\n","OUTPUT_NAME = \"MikaGPT\" #@param {type: \"string\"}\n","#@markdown Enter a number for the number of steps for each stored checkpoint. Enter -1 if you want to save only the last checkpoint\n","SAVE_STEPS = 100 #@param {type: \"integer\"}\n","#@markdown Enter the batch size for each device. Batch size is the sample dataset that is given to the model in one iteration. The more batch sizes, the smaller the gradient variations and the greater the GPU usage. On the other hand, if it is too small, the gradient variation will increase but may cause convergence\n","BATCH_SIZE = 11 #@param {type:\"slider\", min:1, max:64, step:1}\n","#@markdown Enter the number of Epochs for the Fine-Tuning process. Epochs are the number of iterations of one dataset given to the model. The more, the process will take longer\n","TRAIN_EPOCHS = 3 #@param {type:\"slider\", min:1, max:100, step:1}\n","#@markdown Select if you want to continue model training from the checkpoint\n","RESUME_FROM_CHECKPOINT = False #@param {type:\"boolean\"}\n","#@markdown Enter additional parameters, leave blank if there are no additional parameters. For documentation please run cell 0.5 or 1.6\n","ADDITIONAL_PARAMETERS = \"\" #@param {type: \"string\"}\n","\n","if RESUME_FROM_CHECKPOINT:\n","  ADDITIONAL_PARAMETERS += f\" --resume_from_checkpoint {MODEL}\"\n","\n","!pip install -r requirements.txt\n","!python run_clm.py --model_name_or_path {MODEL} --train_file train.txt --validation_file valid.txt --per_device_train_batch_size {BATCH_SIZE} --per_device_eval_batch_size {BATCH_SIZE} --do_train --do_eval --output_dir /content/{OUTPUT_NAME} --overwrite_output_dir --save_steps={SAVE_STEPS} --num_train_epochs {TRAIN_EPOCHS} --logging_steps 100 --save_total_limit 2 {ADDITIONAL_PARAMETERS}\n","\n","print(\"[INFO] Success Fine Tuning Model\")"],"metadata":{"id":"n-gUaVmX8mGf","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title 1.5 Save data to Drive\n","#@markdown Enter the checkpoint folder name (example: checkpoint-10000). Enter \"-\" if you only want to save the last checkpoint\n","STEP = \"-\" #@param {type: \"string\"}\n","\n","%cd ../../../..\n","!mkdir /content/Drive/MyDrive/{OUTPUT_NAME}\n","\n","if STEP != \"-\":\n","  !cp -r {OUTPUT_NAME}/{STEP}/config.json /content/Drive/MyDrive/{OUTPUT_NAME}\n","  !cp -r {OUTPUT_NAME}/{STEP}/generation_config.json /content/Drive/MyDrive/{OUTPUT_NAME}\n","  !cp -r {OUTPUT_NAME}/{STEP}/merges.txt /content/Drive/MyDrive/{OUTPUT_NAME}\n","  !cp -r {OUTPUT_NAME}/{STEP}/pytorch_model.bin /content/Drive/MyDrive/{OUTPUT_NAME}\n","  !cp -r {OUTPUT_NAME}/{STEP}/tokenizer.json /content/Drive/MyDrive/{OUTPUT_NAME}\n","  !cp -r {OUTPUT_NAME}/{STEP}/vocab.json /content/Drive/MyDrive/{OUTPUT_NAME}\n","else:\n","  !cp -r {OUTPUT_NAME}/config.json /content/Drive/MyDrive/{OUTPUT_NAME}\n","  !cp -r {OUTPUT_NAME}/generation_config.json /content/Drive/MyDrive/{OUTPUT_NAME}\n","  !cp -r {OUTPUT_NAME}/merges.txt /content/Drive/MyDrive/{OUTPUT_NAME}\n","  !cp -r {OUTPUT_NAME}/pytorch_model.bin /content/Drive/MyDrive/{OUTPUT_NAME}\n","  !cp -r {OUTPUT_NAME}/tokenizer.json /content/Drive/MyDrive/{OUTPUT_NAME}\n","  !cp -r {OUTPUT_NAME}/vocab.json /content/Drive/MyDrive/{OUTPUT_NAME}\n","\n","print(\"[INFO] Success saving data to Google Drive\")"],"metadata":{"id":"vKtk9QfHO_Rf","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title 1.6 Show help for additional parameters\n","!pip install -r requirements.txt\n","!python run_clm.py --help"],"metadata":{"cellView":"form","id":"7SSejG7hZtvo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **2.0 Inference**\n","Inference is the process of using a previously trained machine learning model to perform certain predictions or tasks on new data. In inference, the model takes new input and produces the expected output based on the knowledge that has been obtained during the training process. The main goal is to apply trained models to solve real-world problems and produce useful results in unfamiliar situations."],"metadata":{"id":"L9H5XkYM6PuE"}},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"OiP7dz_h-zVt"},"outputs":[],"source":["#@title 2.1 Mount Drive\n","\n","import os\n","from google.colab import drive\n","\n","#os.mkdir(\"Drive\")\n","drive.mount(\"Drive\")\n","\n","print(\"[INFO] Success mount drive at /content/Drive\")"]},{"cell_type":"code","source":["#@title 2.2 Install Transformers 4.30.0\n","#@markdown Install Transformers for inference\n","!pip install transformers==4.30.0\n","print(\"[INFO] Please restart runtime to apply changes \")"],"metadata":{"cellView":"form","id":"mJHTmpZLmLgf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title 2.3 Inference model\n","#@markdown Enter the path folder where the model is located or select available models\n","MODEL = \"openai-gpt\" #@param [\"openai-gpt\", \"distilgpt2\", \"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\", \"facebook/opt-125m\", \"facebook/opt-350m\", \"facebook/opt-2.7b\", \"facebook/xglm-564M\", \"facebook/xglm-7.5B\", \"EleutherAI/gpt-neo-125m\", \"EleutherAI/gpt-neo-1.3B\", \"EleutherAI/gpt-j-6b\", \"EleutherAI/pythia-14m\", \"EleutherAI/pythia-31m\", \"EleutherAI/pythia-70m\", \"EleutherAI/pythia-160m\", \"EleutherAI/pythia-410m\" , \"EleutherAI/pythia-1b\", \"roneneldan/TinyStories-1M\", \"roneneldan/TinyStories-3M\", \"roneneldan/TinyStories-8M\", \"roneneldan/TinyStories-33M\"] {allow-input: true}\n","#@markdown Enter the prompt text that you want the model to generate\n","INPUT = \"\" #@param {type: \"string\"}\n","#@markdown Enter the maximum word length to be generated by the model\n","MAX_LENGTH = 200 #@param {type:\"slider\", min:25, max:1000, step:1}\n","\n","from transformers import pipeline\n","model = pipeline(\"text-generation\", model=MODEL)\n","teks = model(INPUT, max_length=MAX_LENGTH)\n","print(teks[0][\"generated_text\"])"],"metadata":{"id":"fPncNiBB8iMF","cellView":"form"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"private_outputs":true,"gpuType":"T4","collapsed_sections":["jIx-FK4bfTld","wDw-qjupSPnb","L9H5XkYM6PuE"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}